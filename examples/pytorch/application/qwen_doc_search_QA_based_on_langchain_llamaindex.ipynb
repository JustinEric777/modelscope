{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a407764-9392-48ae-9bed-8c73c9f76fbc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-16T08:58:56.323000Z",
     "iopub.status.busy": "2024-01-16T08:58:56.322690Z",
     "iopub.status.idle": "2024-01-16T08:59:57.862755Z",
     "shell.execute_reply": "2024-01-16T08:59:57.862041Z",
     "shell.execute_reply.started": "2024-01-16T08:58:56.322980Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirrors.aliyun.com/pypi/simple\n",
      "Collecting pypdf\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/29/10/055b649e914ad8c5d07113c22805014988825abbeff007b0e89255b481fa/pypdf-3.17.4-py3-none-any.whl (278 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m634.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting langchain\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/23/98/c70fac0f1b3193ced86013b563119c27c68ac26b684815f407555224108d/langchain-0.1.0-py3-none-any.whl (797 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m798.0/798.0 kB\u001b[0m \u001b[31m651.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting unstructured\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/7e/46/0f1105b77dcabc9cacb8e0767b3ed68b2078da3d52c44b7799def7403443/unstructured-0.12.0-py3-none-any.whl (1.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m650.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: transformers_stream_generator in /opt/conda/lib/python3.10/site-packages (0.0.4)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.10/site-packages (from langchain) (6.0.1)\n",
      "Collecting SQLAlchemy<3,>=1.4 (from langchain)\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/2c/e6/967cd898cbce485c385d4cd644195f906b2571f9393dc1537019a821a8a6/SQLAlchemy-2.0.25-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m641.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/lib/python3.10/site-packages (from langchain) (3.9.1)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from langchain) (4.0.3)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/ae/53/8c006de775834cd4ea64a445402dc195caeebb77dc76b7defb9b3887cb0d/dataclasses_json-0.6.3-py3-none-any.whl (28 kB)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain)\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/73/07/02e16ed01e04a374e644b575638ec7987ae846d25ad97bcc9945a3ee4b0e/jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Collecting langchain-community<0.1,>=0.0.9 (from langchain)\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/80/18/33bf210e5410289b76da2862e6bcaf2217ee8c42ed0857e02741c4c02431/langchain_community-0.0.12-py3-none-any.whl (1.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m641.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting langchain-core<0.2,>=0.1.7 (from langchain)\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/5c/b9/95f91c284a9c316dbc1c6147076deede8452f147a067762828b125b50352/langchain_core-0.1.10-py3-none-any.whl (216 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.6/216.6 kB\u001b[0m \u001b[31m643.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting langsmith<0.1.0,>=0.0.77 (from langchain)\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/f9/ed/1367bf56ad15c56f2cad40077ea6f9f040fc5939ed0fae28028fd67dbba2/langsmith-0.0.80-py3-none-any.whl (48 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.3/48.3 kB\u001b[0m \u001b[31m675.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain) (1.26.1)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain) (1.10.13)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /opt/conda/lib/python3.10/site-packages (from langchain) (8.2.3)\n",
      "Collecting chardet (from unstructured)\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/38/6f/f5fbc992a329ee4e0f288c1fe0e2ad9485ed064cac731ed2fe47dcc38cbf/chardet-5.2.0-py3-none-any.whl (199 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.4/199.4 kB\u001b[0m \u001b[31m658.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting filetype (from unstructured)\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/18/79/1b8fa1bb3568781e84c9200f951c735f3f157429f44be0495da55894d620/filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
      "Collecting python-magic (from unstructured)\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/6c/73/9f872cb81fc5c3bb48f7227872c28975f998f3e7c2b1c16e95e6432bbb90/python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: lxml in /opt/conda/lib/python3.10/site-packages (from unstructured) (4.9.3)\n",
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from unstructured) (3.8.1)\n",
      "Requirement already satisfied: tabulate in /opt/conda/lib/python3.10/site-packages (from unstructured) (0.9.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.10/site-packages (from unstructured) (4.12.2)\n",
      "Requirement already satisfied: emoji in /opt/conda/lib/python3.10/site-packages (from unstructured) (2.9.0)\n",
      "Collecting python-iso639 (from unstructured)\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/7a/f2/f8ab4578365479cb3018a998742c42699f22df6be1ba3c803185c8f51836/python_iso639-2024.1.2-py3-none-any.whl (274 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m274.7/274.7 kB\u001b[0m \u001b[31m659.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting langdetect (from unstructured)\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/0e/72/a3add0e4eec4eb9e2569554f7c70f4a3c27712f40e3284d483e88094cc0e/langdetect-1.0.9.tar.gz (981 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m676.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: rapidfuzz in /opt/conda/lib/python3.10/site-packages (from unstructured) (3.5.2)\n",
      "Collecting backoff (from unstructured)\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/df/73/b6e24bd22e6720ca8ee9a85a0c4a2971af8497d8f3193fa05390cbd46e09/backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from unstructured) (4.8.0)\n",
      "Collecting unstructured-client (from unstructured)\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/f0/63/d76168e51d0ad637eda5f7952ac7e67a12949656056981450be2e381e77c/unstructured_client-0.15.2-py3-none-any.whl (20 kB)\n",
      "Requirement already satisfied: wrapt in /opt/conda/lib/python3.10/site-packages (from unstructured) (1.14.1)\n",
      "Requirement already satisfied: transformers>=4.26.1 in /opt/conda/lib/python3.10/site-packages (from transformers_stream_generator) (4.34.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/57/e9/4368d49d3b462da16a3bac976487764a84dd85cef97232c7bd61f5bdedf3/marshmallow-3.20.2-py3-none-any.whl (49 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m708.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/65/f3/107a22063bf27bdccf2024833d3445f4eea42b2e598abfbd46f6a63b6cb0/typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.1)\n",
      "Requirement already satisfied: anyio<5,>=3 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.2,>=0.1.7->langchain) (3.7.1)\n",
      "Collecting packaging<24.0,>=23.2 (from langchain-core<0.2,>=0.1.7->langchain)\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/ec/1a/610693ac4ee14fcdf2d9bf3c493370e4f2ef7ae2e19217d7a237ff42367d/packaging-23.2-py3-none-any.whl (53 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m687.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2023.7.22)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers>=4.26.1->transformers_stream_generator) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.26.1->transformers_stream_generator) (0.19.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.26.1->transformers_stream_generator) (2023.10.3)\n",
      "Requirement already satisfied: tokenizers<0.15,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.26.1->transformers_stream_generator) (0.14.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.26.1->transformers_stream_generator) (0.4.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.26.1->transformers_stream_generator) (4.65.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4->unstructured) (2.5)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from langdetect->unstructured) (1.16.0)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from nltk->unstructured) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from nltk->unstructured) (1.3.2)\n",
      "Collecting charset-normalizer<4,>=2 (from requests<3,>=2->langchain)\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/da/f1/3702ba2a7470666a62fd81c58a4c40be00670e5006a67f4d626e57f013ae/charset_normalizer-3.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.1/142.1 kB\u001b[0m \u001b[31m658.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting jsonpath-python>=1.0.6 (from unstructured-client->unstructured)\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/16/8a/d63959f4eff03893a00e6e63592e3a9f15b9266ed8e0275ab77f8c7dbc94/jsonpath_python-1.0.6-py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: mypy-extensions>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from unstructured-client->unstructured) (1.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from unstructured-client->unstructured) (2.8.2)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests<3,>=2->langchain)\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/96/94/c31f58c7a7f470d5665935262ebd7455c7e4c7782eb525658d3dbf4b9403/urllib3-2.1.0-py3-none-any.whl (104 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.6/104.6 kB\u001b[0m \u001b[31m689.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.7->langchain) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.7->langchain) (1.1.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers>=4.26.1->transformers_stream_generator) (2023.10.0)\n",
      "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers>=4.26.1->transformers_stream_generator)\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/aa/f3/3fc97336a0e90516901befd4f500f08d691034d387406fdbde85bea827cc/huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m603.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: langdetect\n",
      "  Building wheel for langdetect (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993243 sha256=fd588d4105f3490a47f762d125ac10eda54f7e1cf3cffd63c4176a75309feea8\n",
      "  Stored in directory: /root/.cache/pip/wheels/6f/b0/12/9ae00df8717763e321f27d2d37fee04d7765769bdb2d60a7c7\n",
      "Successfully built langdetect\n",
      "\u001b[33mDEPRECATION: omegaconf 2.0.6 has a non-standard dependency specifier PyYAML>=5.1.*. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of omegaconf or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: pytorch-lightning 1.7.7 has a non-standard dependency specifier torch>=1.9.*. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pytorch-lightning or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: filetype, urllib3, typing-inspect, SQLAlchemy, python-magic, python-iso639, pypdf, packaging, langdetect, jsonpath-python, jsonpatch, charset-normalizer, chardet, backoff, marshmallow, langsmith, huggingface-hub, dataclasses-json, unstructured-client, langchain-core, unstructured, langchain-community, langchain\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.26.16\n",
      "    Uninstalling urllib3-1.26.16:\n",
      "      Successfully uninstalled urllib3-1.26.16\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 23.1\n",
      "    Uninstalling packaging-23.1:\n",
      "      Successfully uninstalled packaging-23.1\n",
      "  Attempting uninstall: jsonpatch\n",
      "    Found existing installation: jsonpatch 1.32\n",
      "    Uninstalling jsonpatch-1.32:\n",
      "      Successfully uninstalled jsonpatch-1.32\n",
      "  Attempting uninstall: charset-normalizer\n",
      "    Found existing installation: charset-normalizer 2.0.4\n",
      "    Uninstalling charset-normalizer-2.0.4:\n",
      "      Successfully uninstalled charset-normalizer-2.0.4\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.19.4\n",
      "    Uninstalling huggingface-hub-0.19.4:\n",
      "      Successfully uninstalled huggingface-hub-0.19.4\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "botocore 1.33.8 requires urllib3<2.1,>=1.25.4; python_version >= \"3.10\", but you have urllib3 2.1.0 which is incompatible.\n",
      "conda 23.9.0 requires ruamel-yaml<0.18,>=0.11.14, but you have ruamel-yaml 0.18.5 which is incompatible.\n",
      "datasets 2.15.0 requires huggingface-hub>=0.18.0, but you have huggingface-hub 0.17.3 which is incompatible.\n",
      "detectron2 0.6 requires hydra-core>=1.1, but you have hydra-core 1.0.7 which is incompatible.\n",
      "detectron2 0.6 requires omegaconf<2.4,>=2.1, but you have omegaconf 2.0.6 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed SQLAlchemy-2.0.25 backoff-2.2.1 chardet-5.2.0 charset-normalizer-3.3.2 dataclasses-json-0.6.3 filetype-1.2.0 huggingface-hub-0.17.3 jsonpatch-1.33 jsonpath-python-1.0.6 langchain-0.1.0 langchain-community-0.0.12 langchain-core-0.1.10 langdetect-1.0.9 langsmith-0.0.80 marshmallow-3.20.2 packaging-23.2 pypdf-3.17.4 python-iso639-2024.1.2 python-magic-0.4.27 typing-inspect-0.9.0 unstructured-0.12.0 unstructured-client-0.15.2 urllib3-2.1.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Looking in indexes: https://mirrors.aliyun.com/pypi/simple\n",
      "Requirement already satisfied: modelscope in /opt/conda/lib/python3.10/site-packages (1.10.0)\n",
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (3.8.1)\n",
      "Requirement already satisfied: pydantic in /opt/conda/lib/python3.10/site-packages (1.10.13)\n",
      "Requirement already satisfied: tiktoken in /opt/conda/lib/python3.10/site-packages (0.5.1)\n",
      "Collecting llama-index\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/81/87/2a92bd45ea524ea4e3f44cb2d97a6820319fcc42978f45d8fe09620ee680/llama_index-0.9.31-py3-none-any.whl (15.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.8/15.8 MB\u001b[0m \u001b[31m680.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: addict in /opt/conda/lib/python3.10/site-packages (from modelscope) (2.4.0)\n",
      "Requirement already satisfied: attrs in /opt/conda/lib/python3.10/site-packages (from modelscope) (23.1.0)\n",
      "Requirement already satisfied: datasets>=2.14.5 in /opt/conda/lib/python3.10/site-packages (from modelscope) (2.15.0)\n",
      "Requirement already satisfied: einops in /opt/conda/lib/python3.10/site-packages (from modelscope) (0.7.0)\n",
      "Requirement already satisfied: filelock>=3.3.0 in /opt/conda/lib/python3.10/site-packages (from modelscope) (3.13.1)\n",
      "Requirement already satisfied: gast>=0.2.2 in /opt/conda/lib/python3.10/site-packages (from modelscope) (0.5.4)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from modelscope) (1.26.1)\n",
      "Requirement already satisfied: oss2 in /opt/conda/lib/python3.10/site-packages (from modelscope) (2.18.3)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from modelscope) (2.1.3)\n",
      "Requirement already satisfied: Pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from modelscope) (10.1.0)\n",
      "Requirement already satisfied: pyarrow!=9.0.0,>=6.0.0 in /opt/conda/lib/python3.10/site-packages (from modelscope) (14.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.10/site-packages (from modelscope) (2.8.2)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from modelscope) (6.0.1)\n",
      "Requirement already satisfied: requests>=2.25 in /opt/conda/lib/python3.10/site-packages (from modelscope) (2.31.0)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from modelscope) (1.11.3)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from modelscope) (68.0.0)\n",
      "Requirement already satisfied: simplejson>=3.3.0 in /opt/conda/lib/python3.10/site-packages (from modelscope) (3.19.2)\n",
      "Requirement already satisfied: sortedcontainers>=1.5.9 in /opt/conda/lib/python3.10/site-packages (from modelscope) (2.4.0)\n",
      "Requirement already satisfied: tqdm>=4.64.0 in /opt/conda/lib/python3.10/site-packages (from modelscope) (4.65.0)\n",
      "Requirement already satisfied: urllib3>=1.26 in /opt/conda/lib/python3.10/site-packages (from modelscope) (2.1.0)\n",
      "Requirement already satisfied: yapf in /opt/conda/lib/python3.10/site-packages (from modelscope) (0.30.0)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.10/site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/lib/python3.10/site-packages (from pydantic) (4.8.0)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index) (2.0.25)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /opt/conda/lib/python3.10/site-packages (from llama-index) (3.9.1)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.2 in /opt/conda/lib/python3.10/site-packages (from llama-index) (4.12.2)\n",
      "Requirement already satisfied: dataclasses-json in /opt/conda/lib/python3.10/site-packages (from llama-index) (0.6.3)\n",
      "Collecting deprecated>=1.2.9.3 (from llama-index)\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/20/8d/778b7d51b981a96554f29136cd59ca7880bf58094338085bcf2a979a0e6a/Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from llama-index) (2023.10.0)\n",
      "Collecting httpx (from llama-index)\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/39/9b/4937d841aee9c2c8102d9a4eeb800c7dad25386caabb4a1bf5010df81a57/httpx-0.26.0-py3-none-any.whl (75 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.9/75.9 kB\u001b[0m \u001b[31m710.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /opt/conda/lib/python3.10/site-packages (from llama-index) (1.5.8)\n",
      "Requirement already satisfied: networkx>=3.0 in /opt/conda/lib/python3.10/site-packages (from llama-index) (3.2.1)\n",
      "Collecting openai>=1.1.0 (from llama-index)\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/6a/54/e0af4b74ebb732bfa9bc83d3e49e577d4e332990742a9ecbe228c532a02d/openai-1.7.2-py3-none-any.whl (212 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.1/212.1 kB\u001b[0m \u001b[31m696.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tenacity<9.0.0,>=8.2.0 in /opt/conda/lib/python3.10/site-packages (from llama-index) (8.2.3)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from llama-index) (0.9.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index) (1.9.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index) (4.0.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4<5.0.0,>=4.12.2->llama-index) (2.5)\n",
      "Requirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets>=2.14.5->modelscope) (0.6)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.14.5->modelscope) (0.3.6)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets>=2.14.5->modelscope) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets>=2.14.5->modelscope) (0.70.14)\n",
      "Collecting huggingface-hub>=0.18.0 (from datasets>=2.14.5->modelscope)\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/3d/0a/aed3253a9ce63d9c90829b1d36bc44ad966499ff4f5827309099c8c9184b/huggingface_hub-0.20.2-py3-none-any.whl (330 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m330.3/330.3 kB\u001b[0m \u001b[31m689.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets>=2.14.5->modelscope) (23.2)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /opt/conda/lib/python3.10/site-packages (from deprecated>=1.2.9.3->llama-index) (1.14.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/conda/lib/python3.10/site-packages (from openai>=1.1.0->llama-index) (3.7.1)\n",
      "Collecting distro<2,>=1.7.0 (from openai>=1.1.0->llama-index)\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/12/b3/231ffd4ab1fc9d679809f356cebee130ac7daa00d6d6f3206dd4fd137e9e/distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Requirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from openai>=1.1.0->llama-index) (1.3.0)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx->llama-index) (2023.7.22)\n",
      "Collecting httpcore==1.* (from httpx->llama-index)\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/56/ba/78b0a99c4da0ff8b0f59defa2f13ca4668189b134bd9840b6202a93d9a0f/httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m685.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: idna in /opt/conda/lib/python3.10/site-packages (from httpx->llama-index) (3.4)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx->llama-index) (0.14.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.1->modelscope) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.25->modelscope) (3.3.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index) (3.0.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from typing-inspect>=0.8.0->llama-index) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json->llama-index) (3.20.2)\n",
      "Requirement already satisfied: crcmod>=1.7 in /opt/conda/lib/python3.10/site-packages (from oss2->modelscope) (1.7)\n",
      "Requirement already satisfied: pycryptodome>=3.4.7 in /opt/conda/lib/python3.10/site-packages (from oss2->modelscope) (3.19.0)\n",
      "Requirement already satisfied: aliyun-python-sdk-kms>=2.4.1 in /opt/conda/lib/python3.10/site-packages (from oss2->modelscope) (2.16.2)\n",
      "Requirement already satisfied: aliyun-python-sdk-core>=2.13.12 in /opt/conda/lib/python3.10/site-packages (from oss2->modelscope) (2.14.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->modelscope) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->modelscope) (2023.3)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.9.3 in /opt/conda/lib/python3.10/site-packages (from aliyun-python-sdk-core>=2.13.12->oss2->modelscope) (0.10.0)\n",
      "Requirement already satisfied: cryptography>=2.6.0 in /opt/conda/lib/python3.10/site-packages (from aliyun-python-sdk-core>=2.13.12->oss2->modelscope) (41.0.3)\n",
      "Requirement already satisfied: exceptiongroup in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai>=1.1.0->llama-index) (1.1.3)\n",
      "Requirement already satisfied: cffi>=1.12 in /opt/conda/lib/python3.10/site-packages (from cryptography>=2.6.0->aliyun-python-sdk-core>=2.13.12->oss2->modelscope) (1.15.1)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.10/site-packages (from cffi>=1.12->cryptography>=2.6.0->aliyun-python-sdk-core>=2.13.12->oss2->modelscope) (2.21)\n",
      "\u001b[33mDEPRECATION: omegaconf 2.0.6 has a non-standard dependency specifier PyYAML>=5.1.*. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of omegaconf or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: pytorch-lightning 1.7.7 has a non-standard dependency specifier torch>=1.9.*. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pytorch-lightning or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: httpcore, distro, deprecated, huggingface-hub, httpx, openai, llama-index\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.17.3\n",
      "    Uninstalling huggingface-hub-0.17.3:\n",
      "      Successfully uninstalled huggingface-hub-0.17.3\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tokenizers 0.14.1 requires huggingface_hub<0.18,>=0.16.4, but you have huggingface-hub 0.20.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed deprecated-1.2.14 distro-1.9.0 httpcore-1.0.2 httpx-0.26.0 huggingface-hub-0.20.2 llama-index-0.9.31 openai-1.7.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pypdf langchain unstructured transformers_stream_generator\n",
    "!pip install modelscope  nltk pydantic  tiktoken  llama-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "696c6b78-53e8-4135-8376-ce8902b7d79a",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-01-16T09:04:59.193375Z",
     "iopub.status.busy": "2024-01-16T09:04:59.193082Z",
     "iopub.status.idle": "2024-01-16T09:05:00.971449Z",
     "shell.execute_reply": "2024-01-16T09:05:00.970857Z",
     "shell.execute_reply.started": "2024-01-16T09:04:59.193357Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  punkt.zip\n",
      "   creating: punkt/\n",
      "  inflating: punkt/greek.pickle      \n",
      "  inflating: punkt/estonian.pickle   \n",
      "  inflating: punkt/turkish.pickle    \n",
      "  inflating: punkt/polish.pickle     \n",
      "   creating: punkt/PY3/\n",
      "  inflating: punkt/PY3/greek.pickle  \n",
      "  inflating: punkt/PY3/estonian.pickle  \n",
      "  inflating: punkt/PY3/turkish.pickle  \n",
      "  inflating: punkt/PY3/polish.pickle  \n",
      "  inflating: punkt/PY3/russian.pickle  \n",
      "  inflating: punkt/PY3/czech.pickle  \n",
      "  inflating: punkt/PY3/portuguese.pickle  \n",
      "  inflating: punkt/PY3/README        \n",
      "  inflating: punkt/PY3/dutch.pickle  \n",
      "  inflating: punkt/PY3/norwegian.pickle  \n",
      "  inflating: punkt/PY3/slovene.pickle  \n",
      "  inflating: punkt/PY3/english.pickle  \n",
      "  inflating: punkt/PY3/danish.pickle  \n",
      "  inflating: punkt/PY3/finnish.pickle  \n",
      "  inflating: punkt/PY3/swedish.pickle  \n",
      "  inflating: punkt/PY3/spanish.pickle  \n",
      "  inflating: punkt/PY3/german.pickle  \n",
      "  inflating: punkt/PY3/italian.pickle  \n",
      "  inflating: punkt/PY3/french.pickle  \n",
      "  inflating: punkt/russian.pickle    \n",
      "  inflating: punkt/czech.pickle      \n",
      "  inflating: punkt/portuguese.pickle  \n",
      "  inflating: punkt/README            \n",
      "  inflating: punkt/dutch.pickle      \n",
      "  inflating: punkt/norwegian.pickle  \n",
      "  inflating: punkt/slovene.pickle    \n",
      "  inflating: punkt/english.pickle    \n",
      "  inflating: punkt/danish.pickle     \n",
      "  inflating: punkt/finnish.pickle    \n",
      "  inflating: punkt/swedish.pickle    \n",
      "  inflating: punkt/spanish.pickle    \n",
      "  inflating: punkt/german.pickle     \n",
      "  inflating: punkt/italian.pickle    \n",
      "  inflating: punkt/french.pickle     \n",
      "  inflating: punkt/.DS_Store         \n",
      "  inflating: punkt/PY3/malayalam.pickle  \n",
      "  inflating: punkt/malayalam.pickle  \n",
      "Archive:  averaged_perceptron_tagger.zip\n",
      "   creating: averaged_perceptron_tagger/\n",
      "  inflating: averaged_perceptron_tagger/averaged_perceptron_tagger.pickle  \n"
     ]
    }
   ],
   "source": [
    "!mkdir -p /root/nltk_data/tokenizers\n",
    "!mkdir -p /root/nltk_data/taggers\n",
    "!cp /mnt/workspace/punkt.zip /root/nltk_data/tokenizers\n",
    "!cp /mnt/workspace/averaged_perceptron_tagger.zip /root/nltk_data/taggers\n",
    "!cd /root/nltk_data/tokenizers; unzip punkt.zip;\n",
    "!cd /root/nltk_data/taggers; unzip averaged_perceptron_tagger.zip;\n",
    "\n",
    "!mkdir -p /mnt/workspace/custom_data\n",
    "!mv /mnt/workspace/xianjiaoda.md /mnt/workspace/custom_data\n",
    "\n",
    "!cd /mnt/workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1cb8feca-c71f-4ad6-8eff-caae95411aa0",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-01-16T09:06:03.024995Z",
     "iopub.status.busy": "2024-01-16T09:06:03.024622Z",
     "iopub.status.idle": "2024-01-16T09:09:15.894774Z",
     "shell.execute_reply": "2024-01-16T09:09:15.894230Z",
     "shell.execute_reply.started": "2024-01-16T09:06:03.024974Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-16 17:06:05,058 - modelscope - INFO - PyTorch version 2.1.0+cu118 Found.\n",
      "2024-01-16 17:06:05,060 - modelscope - INFO - TensorFlow version 2.14.0 Found.\n",
      "2024-01-16 17:06:05,061 - modelscope - INFO - Loading ast index from /mnt/workspace/.cache/modelscope/ast_indexer\n",
      "2024-01-16 17:06:05,061 - modelscope - INFO - No valid ast index found from /mnt/workspace/.cache/modelscope/ast_indexer, generating ast index from prebuilt!\n",
      "2024-01-16 17:06:05,105 - modelscope - INFO - Loading done! Current index file version is 1.10.0, with md5 44f0b88effe82ceea94a98cf99709694 and a total number of 946 components indexed\n",
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-01-16 17:06:06.173594: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-01-16 17:06:06.206324: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-16 17:06:06.206348: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-16 17:06:06.206366: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-16 17:06:06.212816: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-16 17:06:06.853747: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2024-01-16 17:06:09,232 - modelscope - WARNING - Using the master branch is fragile, please use it with caution!\n",
      "2024-01-16 17:06:09,233 - modelscope - INFO - Use user-specified model revision: master\n",
      "Downloading: 100%|██████████| 8.21k/8.21k [00:00<00:00, 28.2MB/s]\n",
      "Downloading: 100%|██████████| 50.8k/50.8k [00:00<00:00, 6.27MB/s]\n",
      "Downloading: 100%|██████████| 244k/244k [00:00<00:00, 10.6MB/s]\n",
      "Downloading: 100%|██████████| 135k/135k [00:00<00:00, 5.47MB/s]\n",
      "Downloading: 100%|██████████| 910/910 [00:00<00:00, 6.72MB/s]\n",
      "Downloading: 100%|██████████| 77.0/77.0 [00:00<00:00, 559kB/s]\n",
      "Downloading: 100%|██████████| 2.29k/2.29k [00:00<00:00, 13.3MB/s]\n",
      "Downloading: 100%|██████████| 1.88k/1.88k [00:00<00:00, 11.9MB/s]\n",
      "Downloading: 100%|██████████| 249/249 [00:00<00:00, 1.66MB/s]\n",
      "Downloading: 100%|██████████| 1.63M/1.63M [00:00<00:00, 22.6MB/s]\n",
      "Downloading: 100%|██████████| 1.84M/1.84M [00:00<00:00, 18.5MB/s]\n",
      "Downloading: 100%|██████████| 2.64M/2.64M [00:00<00:00, 42.7MB/s]\n",
      "Downloading: 100%|██████████| 7.11k/7.11k [00:00<00:00, 445kB/s]\n",
      "Downloading: 100%|██████████| 80.8k/80.8k [00:00<00:00, 9.97MB/s]\n",
      "Downloading: 100%|██████████| 80.8k/80.8k [00:00<00:00, 24.2MB/s]\n",
      "Downloading: 100%|██████████| 54.3k/54.3k [00:00<00:00, 8.83MB/s]\n",
      "Downloading: 100%|██████████| 15.0k/15.0k [00:00<00:00, 4.45MB/s]\n",
      "Downloading: 100%|██████████| 237k/237k [00:00<00:00, 15.2MB/s]\n",
      "Downloading: 100%|██████████| 116k/116k [00:00<00:00, 28.3MB/s]\n",
      "Downloading: 100%|██████████| 2.44M/2.44M [00:00<00:00, 39.7MB/s]\n",
      "Downloading: 100%|██████████| 473k/473k [00:00<00:00, 38.4MB/s]\n",
      "Downloading: 100%|██████████| 14.3k/14.3k [00:00<00:00, 5.81MB/s]\n",
      "Downloading: 100%|██████████| 79.0k/79.0k [00:00<00:00, 78.8MB/s]\n",
      "Downloading: 100%|██████████| 46.4k/46.4k [00:00<00:00, 32.2MB/s]\n",
      "Downloading: 100%|██████████| 0.98M/0.98M [00:00<00:00, 27.7MB/s]\n",
      "Downloading: 100%|██████████| 205k/205k [00:00<00:00, 9.33MB/s]\n",
      "Downloading: 100%|██████████| 19.4k/19.4k [00:00<00:00, 3.79MB/s]\n",
      "Downloading: 100%|██████████| 302k/302k [00:00<00:00, 32.0MB/s]\n",
      "Downloading: 100%|██████████| 615k/615k [00:00<00:00, 28.8MB/s]\n",
      "Downloading: 100%|██████████| 376k/376k [00:00<00:00, 26.3MB/s]\n",
      "Downloading: 100%|██████████| 445k/445k [00:00<00:00, 15.0MB/s]\n",
      "Downloading: 100%|██████████| 25.9k/25.9k [00:00<00:00, 5.72MB/s]\n",
      "Downloading: 100%|██████████| 395k/395k [00:00<00:00, 19.2MB/s]\n",
      "Downloading: 100%|██████████| 176k/176k [00:00<00:00, 14.1MB/s]\n",
      "Downloading: 100%|██████████| 182k/182k [00:00<00:00, 22.7MB/s]\n",
      "Downloading: 100%|██████████| 824k/824k [00:00<00:00, 11.3MB/s]\n",
      "Downloading: 100%|██████████| 426k/426k [00:00<00:00, 18.8MB/s]\n",
      "Downloading: 100%|██████████| 433k/433k [00:00<00:00, 6.04MB/s]\n",
      "Downloading: 100%|██████████| 466k/466k [00:00<00:00, 14.0MB/s]\n",
      "Downloading: 100%|██████████| 403k/403k [00:00<00:00, 17.4MB/s]\n",
      "Downloading: 100%|██████████| 9.39k/9.39k [00:00<00:00, 34.4MB/s]\n",
      "Downloading: 100%|██████████| 403k/403k [00:00<00:00, 17.5MB/s]\n",
      "Downloading: 100%|██████████| 79.0k/79.0k [00:00<00:00, 6.10MB/s]\n",
      "Downloading: 100%|██████████| 173/173 [00:00<00:00, 1.21MB/s]\n",
      "Downloading: 100%|██████████| 41.9k/41.9k [00:00<00:00, 3.99MB/s]\n",
      "Downloading: 100%|██████████| 230k/230k [00:00<00:00, 38.7MB/s]\n",
      "Downloading: 100%|██████████| 1.27M/1.27M [00:01<00:00, 822kB/s]\n",
      "Downloading: 100%|██████████| 664k/664k [00:00<00:00, 23.3MB/s]\n",
      "Downloading: 100%|██████████| 404k/404k [00:00<00:00, 18.3MB/s]\n",
      "2024-01-16 17:07:26,373 - modelscope - WARNING - Using the master branch is fragile, please use it with caution!\n",
      "2024-01-16 17:07:26,374 - modelscope - INFO - Use user-specified model revision: master\n",
      "Downloading: 100%|█████████▉| 1.90G/1.90G [00:50<00:00, 40.7MB/s]\n",
      "Downloading: 100%|█████████▉| 1.52G/1.52G [00:04<00:00, 335MB/s]\n",
      "Downloading: 100%|██████████| 14.4k/14.4k [00:00<00:00, 4.46MB/s]\n",
      "Try importing flash-attention for faster inference...\n",
      "Warning: import flash_attn rms_norm fail, please install FlashAttention layer_norm to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/layer_norm\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP1: qianwen LLM created\n",
      "STEP2: reading docs ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-16 17:08:33,947 - modelscope - WARNING - Model revision not specified, use revision: v1.0.0\n",
      "Downloading: 100%|██████████| 772/772 [00:00<00:00, 5.78MB/s]\n",
      "Downloading: 100%|██████████| 2.02k/2.02k [00:00<00:00, 15.1MB/s]\n",
      "Downloading: 100%|██████████| 60.7k/60.7k [00:00<00:00, 5.56MB/s]\n",
      "Downloading: 100%|██████████| 57.7M/57.7M [00:00<00:00, 229MB/s]\n",
      "Downloading: 100%|██████████| 15.3k/15.3k [00:00<00:00, 5.51MB/s]\n",
      "Downloading: 100%|██████████| 125/125 [00:00<00:00, 978kB/s]\n",
      "Downloading: 100%|██████████| 291k/291k [00:00<00:00, 27.8MB/s]\n",
      "Downloading: 100%|██████████| 425/425 [00:00<00:00, 1.81MB/s]\n",
      "Downloading: 100%|██████████| 68.4k/68.4k [00:00<00:00, 6.29MB/s]\n",
      "2024-01-16 17:09:10,871 - modelscope - INFO - initiate model from /mnt/workspace/.cache/modelscope/damo/nlp_gte_sentence-embedding_chinese-small\n",
      "2024-01-16 17:09:10,871 - modelscope - INFO - initiate model from location /mnt/workspace/.cache/modelscope/damo/nlp_gte_sentence-embedding_chinese-small.\n",
      "2024-01-16 17:09:10,873 - modelscope - INFO - initialize model from /mnt/workspace/.cache/modelscope/damo/nlp_gte_sentence-embedding_chinese-small\n",
      "2024-01-16 17:09:11,242 - modelscope - WARNING - No preprocessor field found in cfg.\n",
      "2024-01-16 17:09:11,243 - modelscope - WARNING - No val key and type key found in preprocessor domain of configuration.json file.\n",
      "2024-01-16 17:09:11,243 - modelscope - WARNING - Cannot find available config to build preprocessor at mode inference, current config: {'model_dir': '/mnt/workspace/.cache/modelscope/damo/nlp_gte_sentence-embedding_chinese-small'}. trying to build by task and model information.\n",
      "2024-01-16 17:09:11,265 - modelscope - WARNING - No preprocessor field found in cfg.\n",
      "2024-01-16 17:09:11,266 - modelscope - WARNING - No val key and type key found in preprocessor domain of configuration.json file.\n",
      "2024-01-16 17:09:11,266 - modelscope - WARNING - Cannot find available config to build preprocessor at mode inference, current config: {'model_dir': '/mnt/workspace/.cache/modelscope/damo/nlp_gte_sentence-embedding_chinese-small', 'sequence_length': 128}. trying to build by task and model information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM is explicitly disabled. Using MockLLM.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:905: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2.2 reading doc done, vec db created.\n",
      "STEP3: chat prompt template created.\n",
      "@@@ query= 西安交大的校训是什么？\n",
      "@@@@ source= Node ID: 28f7450e-0d60-49b1-a93a-5da17846d3d1\n",
      "Text: 西安交通大学是我国最早兴办、享誉海内外的著名高等学府，是教育部直属重点大学。西迁以来，一代代交大人扎根西部、服务国家，为西部发展\n",
      "和国家建设作出了卓越贡献，以实际行动铸就了第一批纳入中国共产党人精神谱系的西迁精神。2017年12月，习近平总书记对学校15位老教授来信作出\n",
      "重要指示。在2018年新年贺词中，习近平总书记再次提到“西安交大西迁的老教授”。2020年4月22日，习近平总书记来校考察并发表重要讲话，强\n",
      "调西迁精神的核心是爱国主义，精髓是听党指挥跟党走，与党和国家、与民族和人民同呼吸、共命运，勉励师生在新时代创造属于我们这代人的历史功绩，给全\n",
      "校师生以巨大关怀和极大鼓舞，为学校新时代建设中国特色世界一流大学提供了根本遵循和行动指南。\n",
      "十九世纪末，甲午战败，民族危难。近代著名实业家、教育...\n",
      "Score:  0.916\n",
      "\n",
      "@@@@ source= Node ID: c600566c-65ed-4b98-9d3a-ef2992892215\n",
      "Text: 2000年国务院决定将西安交通大学、西安医科大学、陕西财经学院三校合并，组建新的西安交通大学。\n",
      "学校是“七五”“八五”重点建设单位，首批进入国家“211”和“985”工程建设学校。2017 年入选国家一流大学建设名单 A\n",
      "类建设高校，2022 年入选国家第二轮“双一流”建设高校，8 个学科入选“双一流”建设学科。据 ESI 公布的数据，截至 2023 年 5\n",
      "月，学校 17 个学科进入世界学术机构前 1%，5 个学科进入前 1‰，其中工程学进入前万分之一。  学校是涵盖理、工、医、经、管、文、法、\n",
      "哲、艺、教育、交叉等11个学科门类的综合性研究型大学，设有32个学院（部、中心）、9个本科书院和3所直属附属医院。现有在编教工6635人，其\n",
      "中专任教师3789人。师资队伍中入选院士、杰青等国...\n",
      "Score:  0.874\n",
      "\n",
      "Human: 请基于```内的内容回答问题。\"\n",
      "```\n",
      "[Document(page_content='西安交通大学是我国最早兴办、享誉海内外的著名高等学府，是教育部直属重点大学。西迁以来，一代代交大人扎根西部、服务国家，为西部发展和国家建设作出了卓越贡献，以实际行动铸就了第一批纳入中国共产党人精神谱系的西迁精神。2017年12月，习近平总书记对学校15位老教授来信作出重要指示。在2018年新年贺词中，习近平总书记再次提到“西安交大西迁的老教授”。2020年4月22日，习近平总书记来校考察并发表重要讲话，强调西迁精神的核心是爱国主义，精髓是听党指挥跟党走，与党和国家、与民族和人民同呼吸、共命运，勉励师生在新时代创造属于我们这代人的历史功绩，给全校师生以巨大关怀和极大鼓舞，为学校新时代建设中国特色世界一流大学提供了根本遵循和行动指南。\\n\\n十九世纪末，甲午战败，民族危难。近代著名实业家、教育家盛宣怀秉持“自强首在储才，储才必先兴学”的信念，于1896年在上海创建了南洋公学，1921年定名为交通大学。学校坚持“求实学、务实业”办学宗旨，强调“修一等品行、求一等学问、创一等事业、成一等人才”办学目标。至二十世纪二三十年代，成为独具“理工管”特色的著名大学。抗战时期，学校移至租界，内迁重庆，坚持沪渝两地办学，为抵御外侮，不少学生投笔从戎，浴血沙场。解放前夕，师生积极投入民主革命和解放斗争，学校被誉为“民主堡垒”。\\n1955年中央决定交通大学内迁西安；1956年起师生分批迁赴西安；1957年分设为交通大学西安、上海两个部分，实行统一领导；1959年，交通大学西安部分定名为西安交通大学，同年被列为全国十六所重点大学之一。2000年国务院决定将西安交通大学、西安医科大学、陕西财经学院三校合并，组建新的西安交通大学。\\n\\n学校是“七五”“八五”重点建设单位，首批进入国家“211”和“985”工程建设学校。2017 年入选国家一流大学建设名单 A 类建设高校，2022 年入选国家第二轮“双一流”建设高校，8 个学科入选“双一流”建设学科。', metadata={'file_path': '/mnt/workspace/custom_data/xianjiaoda.md', 'file_name': 'xianjiaoda.md', 'file_type': 'text/markdown', 'file_size': 13228, 'creation_date': '2024-01-16', 'last_modified_date': '2024-01-16', 'last_accessed_date': '2024-01-16'}), Document(page_content='2000年国务院决定将西安交通大学、西安医科大学、陕西财经学院三校合并，组建新的西安交通大学。\\n\\n学校是“七五”“八五”重点建设单位，首批进入国家“211”和“985”工程建设学校。2017 年入选国家一流大学建设名单 A 类建设高校，2022 年入选国家第二轮“双一流”建设高校，8 个学科入选“双一流”建设学科。据 ESI 公布的数据，截至 2023 年 5 月，学校 17 个学科进入世界学术机构前 1%，5 个学科进入前 1‰，其中工程学进入前万分之一。\\n\\n学校是涵盖理、工、医、经、管、文、法、哲、艺、教育、交叉等11个学科门类的综合性研究型大学，设有32个学院（部、中心）、9个本科书院和3所直属附属医院。现有在编教工6635人，其中专任教师3789人。师资队伍中入选院士、杰青等国家级各类重大人才工程545人次，获评国家级各类创新团队51个，为国家作出突出贡献并享受政府特殊津贴专家450名，国家级教学名师11名。\\n\\n学校现有学生54760名，其中本科生22407名，研究生29285名，留学生3068名；本科招生专业76个、博士学位授权一级学科36个、硕士学位授权一级学科43个、博士专业学位授权点6个、硕士专业学位授权点29个，博士后流动站30个，国家一级重点学科8个、国家二级重点学科8个、国家重点（培育）学科3个，全国（国家）重点实验室8个，国家工程（技术）研究中心10个，国家产教融合创新平台2个，国家国际科技合作基地5个，国家应用数学中心1个，2011协同创新中心1个、其他省部级及以上重点科研基地195个。\\n\\n建校127年来，形成了兴学强国、艰苦创业、崇德尚实、严谨治学的优良传统，起点高、基础厚、要求严、重实践的办学特色，培养出了一大批卓越的政治家、科学家、社会活动家、教育家、企业家、艺术家、医学专家等，如蔡锷、张元济、蔡元培、黄炎培、邵力子、李叔同、邹韬奋、陆定一、钱学森、张光斗、汪道涵、吴文俊、杨嘉墀、徐光宪、姚桐斌、陈能宽、江泽民、侯宗濂、黄旭华、顾诵芬、丁关根、吴自良、蒋新松、蒋正华、王希季、李金华、韩启德等。', metadata={'file_path': '/mnt/workspace/custom_data/xianjiaoda.md', 'file_name': 'xianjiaoda.md', 'file_type': 'text/markdown', 'file_size': 13228, 'creation_date': '2024-01-16', 'last_modified_date': '2024-01-16', 'last_accessed_date': '2024-01-16'})]\n",
      "```\n",
      "我的问题是：西安交大的校训是什么？。\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:905: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2.], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'西安交通大学校训是：“求实学、务实业”'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from abc import ABC\n",
    "from typing import Any, List, Optional, Dict, cast\n",
    "\n",
    "import torch\n",
    "from langchain_core.language_models.llms import LLM\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from modelscope import AutoModelForCausalLM, AutoTokenizer\n",
    "from llama_index import GPTVectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index import ServiceContext\n",
    "from llama_index.embeddings.base import BaseEmbedding\n",
    "from llama_index import set_global_service_context\n",
    "from langchain_core.retrievers import BaseRetriever\n",
    "from langchain_core.callbacks import CallbackManagerForRetrieverRun\n",
    "from langchain_core.documents import Document\n",
    "from llama_index.retrievers import VectorIndexRetriever\n",
    "\n",
    "\n",
    "# 大模型配置\n",
    "llm_name = \"Qwen/Qwen-1_8B-Chat\"  # 大模型\n",
    "llm_revision = \"master\"  # 大模型的版本\n",
    "\n",
    "# embedding模型配置\n",
    "embedding_model = \"damo/nlp_gte_sentence-embedding_chinese-small\"\n",
    "\n",
    "# 知识库的原始文件路径\n",
    "knowledge_doc_file_dir = \"/mnt/workspace/custom_data/\"\n",
    "knowledge_doc_file_path = knowledge_doc_file_dir + \"xianjiaoda.md\"\n",
    "\n",
    "'''\n",
    "!pip install pypdf langchain unstructured transformers_stream_generator\n",
    "!pip install modelscope  nltk pydantic  tiktoken  llama-index\n",
    "\n",
    "!mkdir -p /root/nltk_data/tokenizers\n",
    "!mkdir -p /root/nltk_data/taggers\n",
    "!cp /mnt/workspace/punkt.zip /root/nltk_data/tokenizers\n",
    "!cp /mnt/workspace/averaged_perceptron_tagger.zip /root/nltk_data/taggers\n",
    "!cd /root/nltk_data/tokenizers; unzip punkt.zip;\n",
    "!cd /root/nltk_data/taggers; unzip averaged_perceptron_tagger.zip;\n",
    "\n",
    "!mkdir -p /mnt/workspace/custom_data\n",
    "!mv /mnt/workspace/xianjiaoda.md /mnt/workspace/custom_data\n",
    "\n",
    "!cd /mnt/workspace\n",
    "'''\n",
    "\n",
    "\n",
    "# 基于llamaIndex的BaseEmbedding封装我们自己的embedding class，以便能够使用modelscope中的embedding模型\n",
    "class ModelScopeEmbeddings4LlamaIndex(BaseEmbedding, ABC):\n",
    "    embed: Any = None\n",
    "    model_id: str = \"damo/nlp_gte_sentence-embedding_chinese-small\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            model_id: str,\n",
    "            **kwargs: Any,\n",
    "    ) -> None:\n",
    "        super().__init__(**kwargs)\n",
    "        try:\n",
    "            from modelscope.models import Model\n",
    "            from modelscope.pipelines import pipeline\n",
    "            from modelscope.utils.constant import Tasks\n",
    "            # 使用modelscope的embedding模型（包含下载）\n",
    "            self.embed = pipeline(Tasks.sentence_embedding, model=self.model_id)\n",
    "\n",
    "        except ImportError as e:\n",
    "            raise ValueError(\n",
    "                \"Could not import some python packages.\" \"Please install it with `pip install modelscope`.\"\n",
    "            ) from e\n",
    "\n",
    "    def _get_query_embedding(self, query: str) -> List[float]:\n",
    "        text = query.replace(\"\\n\", \" \")\n",
    "        inputs = {\"source_sentence\": [text]}\n",
    "        return self.embed(input=inputs)['text_embedding'][0]\n",
    "\n",
    "    def _get_text_embedding(self, text: str) -> List[float]:\n",
    "        text = text.replace(\"\\n\", \" \")\n",
    "        inputs = {\"source_sentence\": [text]}\n",
    "        return self.embed(input=inputs)['text_embedding'][0]\n",
    "\n",
    "    def _get_text_embeddings(self, texts: List[str]) -> List[List[float]]:\n",
    "        texts = list(map(lambda x: x.replace(\"\\n\", \" \"), texts))\n",
    "        inputs = {\"source_sentence\": texts}\n",
    "        return self.embed(input=inputs)['text_embedding']\n",
    "\n",
    "    async def _aget_query_embedding(self, query: str) -> List[float]:\n",
    "        return self._get_query_embedding(query)\n",
    "\n",
    "\n",
    "# 为langchain封装llamaIndex的Retriever（langchain自带的LlamaIndexRetriever的接口与llamaIndex当下的定义不兼容）\n",
    "class LlamaIndexRetriever(BaseRetriever):\n",
    "    index: Any\n",
    "    \"\"\"LlamaIndex index to query.\"\"\"\n",
    "\n",
    "    def _get_relevant_documents(\n",
    "        self, query: str, *, run_manager: CallbackManagerForRetrieverRun\n",
    "    ) -> List[Document]:\n",
    "        \"\"\"Get documents relevant for a query.\"\"\"\n",
    "        try:\n",
    "            from llama_index.indices.base import BaseIndex\n",
    "            from llama_index.response.schema import Response\n",
    "        except ImportError:\n",
    "            raise ImportError(\n",
    "                \"You need to install `pip install llama-index` to use this retriever.\"\n",
    "            )\n",
    "        index = cast(BaseIndex, self.index)\n",
    "        retriever = VectorIndexRetriever(index=index)\n",
    "        print('@@@ query=', query)\n",
    "\n",
    "        response = index.as_query_engine().query(query)\n",
    "        response = cast(Response, response)\n",
    "        # parse source nodes\n",
    "        docs = []\n",
    "        for source_node in response.source_nodes:\n",
    "            print('@@@@ source=', source_node)\n",
    "            metadata = source_node.metadata or {}\n",
    "            docs.append(\n",
    "                Document(page_content=source_node.get_text(), metadata=metadata)\n",
    "            )\n",
    "        return docs\n",
    "\n",
    "def torch_gc():\n",
    "    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "    DEVICE = \"cuda\"\n",
    "    DEVICE_ID = \"0\"\n",
    "    CUDA_DEVICE = f\"{DEVICE}:{DEVICE_ID}\" if DEVICE_ID else DEVICE\n",
    "    a = torch.Tensor([1, 2])\n",
    "    a = a.cuda()\n",
    "    print(a)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        with torch.cuda.device(CUDA_DEVICE):\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.ipc_collect()\n",
    "\n",
    "\n",
    "# 定义全局需要的资源\n",
    "tokenizer = AutoTokenizer.from_pretrained(llm_name, revision=llm_revision, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(llm_name, revision=llm_revision, device_map=\"auto\",\n",
    "                                             trust_remote_code=True, fp16=True).eval()\n",
    "\n",
    "\n",
    "# 基于langchain封装modelscope的LLM，我们可以通过langchain使用modelscope上的所有LLM\n",
    "class QianWenChatLLM(LLM):\n",
    "    max_length = 10000\n",
    "    temperature: float = 0.01\n",
    "    top_p = 0.9\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self):\n",
    "        return \"ChatLLM\"\n",
    "\n",
    "    def _call(\n",
    "            self,\n",
    "            prompt: str,\n",
    "            stop: Optional[List[str]] = None,\n",
    "            run_manager=None,\n",
    "            **kwargs: Any,\n",
    "    ) -> str:\n",
    "        print(prompt)\n",
    "        response, history = model.chat(tokenizer, prompt, history=None)\n",
    "        torch_gc()\n",
    "        return response\n",
    "\n",
    "\n",
    "# STEP1: 创建通义千问的chat llm\n",
    "qwllm = QianWenChatLLM()\n",
    "print('STEP1: qianwen LLM created')\n",
    "\n",
    "# STEP2: 加载知识库文档 并 创建知识库的向量db\n",
    "print('STEP2: reading docs ...')\n",
    "# 创建embedding model，并配置到llamaIndex的context中\n",
    "embeddings = ModelScopeEmbeddings4LlamaIndex(model_id=embedding_model)\n",
    "service_context = ServiceContext.from_defaults(embed_model=embeddings, llm=None)\n",
    "set_global_service_context(service_context)     # 全局配置，可能不是一种好的实践\n",
    "\n",
    "llamaIndex_docs = SimpleDirectoryReader(knowledge_doc_file_dir).load_data()\n",
    "llamaIndex_index = GPTVectorStoreIndex.from_documents(llamaIndex_docs, chunk_size=512)\n",
    "retriever = LlamaIndexRetriever(index=llamaIndex_index)\n",
    "print(' 2.2 reading doc done, vec db created.')\n",
    "\n",
    "# STEP3: 创建chat template\n",
    "prompt_template = \"\"\"请基于```内的内容回答问题。\"\n",
    "```\n",
    "{context}\n",
    "```\n",
    "我的问题是：{question}。\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template=prompt_template)\n",
    "print('STEP3: chat prompt template created.')\n",
    "\n",
    "# STEP4: 创建RAG chain以支持问答\n",
    "chain = (\n",
    "        {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | qwllm\n",
    "        | StrOutputParser()\n",
    ")\n",
    "chain.invoke('西安交大的校训是什么？')\n",
    "# chain.invoke('魔搭社区有哪些模型?')\n",
    "# chain.invoke('modelscope是什么?')\n",
    "# chain.invoke('萧峰和乔峰是什么关系?')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
